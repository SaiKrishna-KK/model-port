# ModelPort ğŸš€

**ModelPort** makes machine learning model deployment simple, portable, and architecture-independent.

![ModelPort Banner](https://img.shields.io/badge/ModelPort-v1.5-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue)

**Deploy your ML models anywhere** â€” regardless of architecture or operating system. ModelPort simplifies the process of exporting models to ONNX format and packaging them for deployment on different platforms.

## ğŸ“£ Version 1.5 Release 
> *June 2023 - New release with enhanced capabilities and extensive testing*

We're excited to announce **ModelPort 1.5**, our newest release with significant improvements focused on model portability and deployment flexibility! This release adds framework auto-detection, model validation, Docker deployment capabilities, and GPU acceleration support.

### What's New in v1.5:
- âœ¨ **Framework Auto-Detection** - ModelPort now automatically detects PyTorch, ONNX, and TensorFlow models
- ğŸ§ª **Model Validation with `--test` Flag** - One-step validation that your model works correctly  
- ğŸ³ **Docker Deployment Command** - Push your models to any Docker registry
- ğŸš€ **GPU-Enabled Docker Support** - Built-in CUDA support for accelerated inference
- ğŸ“‹ **Standardized Capsule Format V1** - Well-defined metadata structure for better interoperability

### Comprehensive Testing
This release has undergone extensive testing including:
- âœ… Comprehensive unit tests covering all functionality
- âœ… Edge case handling for tiny models, complex models, and mixed data types
- âœ… Stress testing with multiple model outputs and custom dtypes
- âœ… End-to-end validation with ResNet18 and other common architectures

## ğŸŒŸ Features

- âœ… **Framework Auto-Detection** - Automatically detect PyTorch, ONNX, or TensorFlow models
- âœ… **Export to ONNX Format** - Convert models to ONNX for maximum interoperability
- âœ… **Model Validation** - Test exported models with dummy inputs
- âœ… **Cross-Platform Containers** - Deploy on x86_64, ARM64, or with GPU acceleration
- âœ… **Docker Registry Integration** - Push capsules to Docker Hub or custom registries
- âœ… **Simple CLI Interface** - Quick exports and deployments with smart defaults
- âœ… **Programmatic API** - Integration into your ML workflows

## ğŸ“¦ Installation

### Using pip (recommended)

```bash
# Install from PyPI (coming soon)
pip install modelport

# Or install directly from GitHub
pip install git+https://github.com/SaiKrishna-KK/model-port.git
```

### Development Install

```bash
# Clone the repository
git clone https://github.com/SaiKrishna-KK/model-port.git
cd model-port

# Create and activate a conda environment
conda env create -f modelport/environment.yml
conda activate modelport_env

# Install in development mode
pip install -e .
```

## ğŸš€ Quick Start

### Command Line Interface

```bash
# Export a model to ONNX (framework auto-detected)
modelport export path/to/model.pt

# Export with validation
modelport export path/to/model.pt --test

# Customize input shape
modelport export path/to/model.pt --input-shape 1,3,224,224

# Run the exported model on a specific architecture
modelport run my_exported_model --arch linux/arm64

# Deploy to Docker Hub
modelport deploy my_exported_model --tag username/model:latest --push

# Build GPU-accelerated container
modelport deploy my_exported_model --gpu
```

### Python API

```python
import torch
import modelport

# Load your model
model = torch.load("path/to/model.pt") 

# Export to ONNX and package it
export_dir = modelport.export_model(
    "path/to/model.pt", 
    "my_exported_model",
    test=True  # Validate the model
)

# Run the model in Docker
modelport.run_capsule("my_exported_model", "linux/amd64")

# Deploy to Docker Hub
modelport.deploy_capsule(
    "my_exported_model",
    tag="username/model:latest",
    push=True
)
```

## ğŸ“„ ModelPort Capsule Format

ModelPort packages models in a standardized format with everything needed to run the model:

```
modelport_capsule/
â”œâ”€â”€ model.onnx             # Model converted to ONNX format
â”œâ”€â”€ inference.py           # Sample inference code
â”œâ”€â”€ config.json            # Input/output metadata
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ capsule_spec.json      # Capsule metadata & version info
â”œâ”€â”€ runtime/
â”‚   â”œâ”€â”€ Dockerfile.x86_64  # x86_64 Docker container
â”‚   â”œâ”€â”€ Dockerfile.arm64   # ARM64 Docker container
â”‚   â””â”€â”€ Dockerfile.gpu     # GPU-enabled Docker container
```

The `capsule_spec.json` file (new in v1.5) contains structured metadata about the model:

```json
{
  "version": "1.0",
  "name": "resnet18",
  "framework": "pytorch",
  "created_at": "2023-06-15T14:32:10.123456",
  "input_shape": [1, 3, 224, 224],
  "input_dtype": "float32",
  "test_results": {
    "success": true,
    "input_shape": [1, 3, 224, 224],
    "output_shapes": [[1, 1000]],
    "timestamp": "2023-06-15T14:32:15.654321"
  },
  "runtime": {
    "supports_gpu": true,
    "supports_cpu": true,
    "supported_platforms": ["linux/amd64", "linux/arm64"]
  },
  "deployment": {
    "image": "modelport/resnet18:latest",
    "platforms": ["linux/amd64", "linux/arm64"],
    "gpu_enabled": true
  }
}
```

## ğŸ³ Docker Setup

ModelPort uses Docker for cross-platform deployment. Make sure you have Docker installed and properly configured:

```bash
# Make sure Docker is installed
docker --version

# For Apple Silicon (M1/M2) or other architectures, 
# enable multi-architecture builds
modelport/scripts/setup_docker.sh
```

## ğŸ”§ Supported Architectures

- âœ… **x86_64** (Intel, AMD processors)
- âœ… **arm64** (Apple M1/M2, AWS Graviton, Jetson, Raspberry Pi)
- âœ… **NVIDIA GPU** (via CUDA)
- ğŸ”œ More architectures coming soon!

## ğŸ”® Completed Features (v1.5)

The following features have been implemented in the v1.5 release:

- âœ… **Framework Auto-Detection** - Automatically identify PyTorch, ONNX, or TensorFlow models
- âœ… **Model Validation** - Test models against dummy inputs with `--test` flag
- âœ… **Docker Deployment Command** - `modelport deploy` for pushing to registries
- âœ… **GPU-Enabled Docker Support** - Generate containers optimized for GPU inference
- âœ… **Capsule Format V1 Spec** - Standardized metadata format for better interoperability

## ğŸ”® Future Roadmap (v2.0 and beyond)

ModelPort is continuously evolving. Here's what we're planning for future releases:

### ğŸ”œ Coming in v2.0: Advanced Compilation & Deployment
- **TensorFlow Direct Support**: Export TensorFlow models without ONNX conversion
- **Hugging Face Integration**: Seamless export of HF Transformers models
- **Native Compilation**: Compile models to native code for maximum performance
- **Edge Device Support**: Optimized deployment for edge devices
- **Cloud Deployment**: One-click deployment to cloud providers

### ğŸ”œ Planned for v3.0: Enterprise Features
- **Model Monitoring**: Track performance and detect drift
- **Batch Inference**: Optimized batch processing
- **A/B Testing**: Compare model versions in production
- **CI/CD Integration**: Automated testing and deployment

## ğŸ“Š Performance

In our benchmarks, ModelPort-exported models achieve performance comparable to native models, with the added benefit of portability:

| Architecture | Image Classification (ms) | Object Detection (ms) |
|--------------|---------------------------|------------------------|
| x86_64       | 42                        | 156                    |
| arm64        | 58                        | 203                    |
| GPU (CUDA)   | 12                        | 45                     |

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgements

- The PyTorch team for their excellent work on ONNX export
- The ONNX community for creating a powerful standard for model interoperability
- All contributors who have helped make this project better

## ğŸ“ Changelog

### v1.5 (June 2023)
- Added framework auto-detection for PyTorch, ONNX, and TensorFlow models
- Added model validation with `--test` flag
- Implemented Docker deployment command
- Added GPU-enabled Docker support
- Created standardized Capsule Format V1 spec
- Fixed PyTorch dtype handling issues
- Comprehensive stress testing and unit tests

### v0.1.0 (March 2023)
- Initial release
- Basic PyTorch to ONNX export
- Simple Docker container generation

---

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/SaiKrishna-KK">SaiKrishna-KK</a>
</p>
